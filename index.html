<!DOCTYPE html>
<!-- saved from url=(0048)http://www.cs.yorku.ca/~kamel/sidd/benchmark.php -->
<html lang="en">
	<head>
		<meta charset="UTF-8">
		<!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->

		<title>Deraining</title>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-119390362-1');
		</script>

		<!--<link rel="stylesheet" href="css/reset.css"/>  -->
		<link rel="stylesheet" href="./files/styles.css">

		<script type="text/javascript" src="./files/scripts.js"></script>

		<!-- for slider -->
		<!-- for slider -->
		<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,700' rel='stylesheet' type='text/css'>

		<script src="./files/modernizr.js"></script>
		
	</head>
	
<!-- BODY部分 -->
	<body>

		<!--<div w3-include-html="header.html"></div>-->

		<!--#include file="header.html" -->

		<!-- HEADER -->		
		<div class="head-container">
	
<div class="head-container">
	<div style="width: 100%; text-align:center;">
		<h2>Towards High-Quality Image Deraining: A Survey and A New Benchmark</h2>
		<p>
		<a href="https://www.eecs.yorku.ca/~kamel/" target="_blank" rel="noopener">Xiang Chen</a><sup>1</sup>
		&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
		<a href="https://www.microsoft.com/en-us/research/people/stevelin/" target="_blank" rel="noopener">Jinshan Pan</a><sup>1</sup>
		&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
		<a href="https://www.eecs.yorku.ca/~mbrown/" target="_blank">San Zhang</a><sup>2</sup>
		</p>

		<p>
		<sup>1</sup><a href="http://eecs.lassonde.yorku.ca/" target="_blank" rel="noopener">Nanjing University of Science and Technology</a>
		&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
		<sup>2</sup><a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="noopener">Google Research</a>

		</p>
	</div>


	<div class="topnav">
			<a href="index.html"><h2>Home</h2></a>
			<a href="publication/index.html"><h2>Survey</h2></a>
			<a href="benchmark.html"><h2>Benchmark</h2></a>
	</div>


</div>			
		<p>
			<a href="http://130.63.97.225/sidd/benchmark_submit.php" class="button-block">Click here to submit your results</a>
		</p>

		<div class="entry grey-back" >
	<i>
	<p>
		This survey and the designed online platform could serve as a reference source for future study and promote the development of the community. The proposed platform and the collected methods, datasets, and comparison results are publicly available and will be regularly updated at this website.
	</p>
	<!--
	<p>
		The benchmark is currently hosted here on this website.
	</p>
	-->
		</i>
</div>
			<figure class="cd-image-container">
				<img src="./img/rain.png" alt="Noisy Image">
				<span class="cd-image-label" data-type="original">Rainy Image</span>

				<div class="cd-resize-img"> <!-- the resizable image on top -->
					<img src="./img/gt.png" alt="Ground Truth Image">
					<span class="cd-image-label" data-type="modified">Ground Truth</span>
				</div>

				<span class="cd-handle"></span>
			</figure> <!-- cd-image-container -->
		</div>

		<!-- /HEADER -->

		<div class="body-container">

			<h1>Abstract</h1>
			<p>
				Image deraining is a typical low-level vision problem in the last decade, which aims to recover the clear image from the observed rainy one. Recently, numerous deep learning-based methods have resorted to diverse types of networks, such as convolutional neural networks (CNNs), generative adversarial networks (GANs), Transformers, diffusion models, etc., as preferable choices compared to traditional prior-based algorithms. In view of the rapid development of this research field, in this paper, we present a comprehensive and timely survey of current deraining approaches. We first summarize the rainy image formulation models, and establish the latest milestone of single image deraining, which goes from prior-based to data-driven. Then, we offer a detailed review of deep image deraining technologies, including network structures, learning strategies, loss functions, benchmark datasets, and evaluation metrics. In addition, we conduct performance comparisons quantitatively and qualitatively. Since there exits the discrepancy in training samples of existing deraining methods, we provide an online platform for the first time, which releases 200GB different deraining results under commonly-used training modes for general users. Finally, we discuss the key challenges and suggest promising future research directions for this research field. This survey and the designed online platform could serve as a reference source for future study and promote the development of the community. The proposed platform and the collected methods, datasets, and comparison results are publicly available and will be regularly updated at https://cschenxiang.github.io/Deraining.
			</p>


			<h1>Papers</h1>
			<p>
				Xiang Chen, Hao Li, Mingqiang Li, Jinshan Pan. "Learning A Sparse Transformer Network for Effective Image Deraining", IEEE Computer Vision and Pattern Recognition (CVPR), June 2023.
			</p>

			<p>
				[<a href="https://arxiv.org/abs/2303.11950" target="_blank" rel="noopener"><b>PDF</b></a>] &nbsp;
				[<a href="#" onclick="javascript:toggleBibtex('SIDDBibtex'); return false;" title="Show/Hide Bibtex"><b>Bibtex</b></a>]
			</p>

			<div id="SIDDBibtex" style="display:none;">
				<code style="white-space: pre-line;">@inproceedings{DRSformer,
					title={Learning A Sparse Transformer Network for Effective Image Deraining}, 
					author={Chen, Xiang and and Li, Hao and Li, Mingqiang and Pan, Jinshan},
					booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
					year={2023}
				}</code>
			</div>

			<h1>Code</h1>
			<p>
				<a href="https://github.com/cschenxiang/DRSformer" target="_blank" rel="noopener"><b>Rain Image Generation</b></a>
			</p>

			<h1>License</h1>
			<p>The dataset and the associated code repositories are under the MIT License.</p>
			
			<h1>Acknowledgment</h1>
			<p>
				This work has been partly supported by the National Key R&D Program of China (No. 2018AAA0102001), the National Natural Science Foundation of China (Nos. U22B2049, U19B2040, 61922043, 61872421, 62272230), and the Fundamental Research Funds for the Central Universities (No. 30920041109).
			</p>

			<h1>Contact</h1>
			<p>
				For any questions, remarks, or comments, please contact: chenxiang@njust.edu.cn
			</p>


			<!--<h1>Authors</h1>-->

		</div>

		<div class="footer">

		</div>

		<!--
		<script>
			includeHTML();
		</script>
		-->

		<script src="./files/jquery-2.1.1.js"></script>
		<script src="./files/jquery.mobile.custom.min.js"></script> <!-- Resource jQuery -->
		<script src="./files/main.js"></script> <!-- Resource jQuery -->

	<div id="footer">
	<div id="footer-text"></div>
    </div>
    <div id = "logo" style="margin-top: 10px; text-align:center">
	    <div align="center" style="margin:auto;padding-top:10px">
            <div style="width:12%">
				<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=UvAnV6DMvsWrDhyqfAW9pOR-O0NWV6F5XtmusegOD9A&cl=ffffff&w=a"></script>
            </div>
           <br>
            </div>
     </div>     
    
  </div>

	</body>

</html>

